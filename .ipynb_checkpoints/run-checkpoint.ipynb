{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfde93c-1d78-4967-b1dd-740b9ba0f5da",
   "metadata": {},
   "source": [
    "# **Train your own model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602227d4-e564-4a88-b5a8-e93fc05029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "#add other arguments as you need\n",
    "#please refer to the original main.py file\n",
    "%run main.py --epochs=30 --use-gpu --balance --ratio-int=2 --f1-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9806c37-9c32-4467-9be9-c0b3db194a58",
   "metadata": {},
   "source": [
    "# **Use our pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622e6f7-4853-4e70-bff0-7dc16cbae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "import pandas as pd\n",
    "from main_compare import EchoDataGen\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from resnet3d import Model\n",
    "\n",
    "label_csv= r\"C:\\Users\\User\\Desktop\\echo_RV\\clean_data\\five_fold\\clean_a4c_0310.csv\" #use your own dataframe\n",
    "\n",
    "\n",
    "def get_test_info(label_csv, fd):\n",
    "    label_df= pd.read_csv(label_csv)\n",
    "    test_df= label_df.loc[label_df['train or val']==fd]\n",
    "    test_df.reset_index(drop= True, inplace= True)\n",
    "    all_echo=[]\n",
    "    for i in range(len(test_df)):\n",
    "        lid= test_df.iloc[i]['lid']\n",
    "        _set= test_df.iloc[i]['train or val']\n",
    "        label= test_df.iloc[i]['label']\n",
    "        echo_tuple= lid, label, _set\n",
    "        all_echo.append(echo_tuple)\n",
    "    return all_echo\n",
    "\n",
    "\n",
    "loaded_model = Model(128, 128, 20)\n",
    "loaded_model.build(input_shape= (32, 20, 128, 128, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042c273-0ffc-4960-b6ee-6a4824fffae7",
   "metadata": {},
   "source": [
    "> **All five-fold validation models are provided, load 1-5 models with different weight as you need**\\\n",
    "*Load all five of them if performance evaluation or average prediction are to made*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841458a-78f3-4433-8ff9-3822f46bfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(r\"./model/A4C_resnet3d_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ac971-bd14-4ab7-8ea8-05c776a85990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "def get_metrics(test_gen, loaded_model):\n",
    "    tf.keras.utils.set_random_seed(123)  # sets seeds for base-python, numpy and tf\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    test_gen.on_epoch_end()\n",
    "    metrics_list=[]\n",
    "    true_pos= []\n",
    "    false_pos= []\n",
    "    false_neg= []\n",
    "    true_neg= []\n",
    "    ##also find true_pos and false_pos\n",
    "    #for batch_number, (echo_batch, label) in enumerate(val_gen):\n",
    "    for batch_number, (echo_batch, label, lids) in enumerate(test_gen):\n",
    "        metrics= np.zeros((2, 32))\n",
    "        out, prob= loaded_model(echo_batch)\n",
    "        metrics[0]= label[:, 0]\n",
    "        metrics[1]= prob[:, 0]\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "        pos_pred= np.where(metrics[1]> 0.5)[0].tolist()\n",
    "        right_prediction= list(filter(lambda x:metrics[0][x]==1, pos_pred))\n",
    "        wrong_prediction= list(filter(lambda x:metrics[0][x]==0, pos_pred)) #label 0 but prob>0.5\n",
    "        if len(right_prediction)!=0:\n",
    "            true_pos.append(list(map(lambda x:lids[int(x)], right_prediction)))\n",
    "        if len(wrong_prediction)!=0:\n",
    "            false_pos.append(list(map(lambda x:lids[int(x)], wrong_prediction)))\n",
    "    \n",
    "        neg_pred= np.where(metrics[1]< 0.5)[0].tolist()\n",
    "        right_prediction= list(filter(lambda x:metrics[0][x]==0, neg_pred))\n",
    "        wrong_prediction= list(filter(lambda x:metrics[0][x]==1, neg_pred)) #label 1 but prob<0.5\n",
    "        if len(right_prediction)!=0:\n",
    "            true_neg.append(list(map(lambda x:lids[int(x)], right_prediction)))\n",
    "        if len(wrong_prediction)!=0:\n",
    "            false_neg.append(list(map(lambda x:lids[int(x)], wrong_prediction)))\n",
    "        \n",
    "    test_metrics= np.concatenate(metrics_list, axis=1)\n",
    "    return test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a922882-ebf2-48a4-b497-5a49c3485c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "#performance is stored in a dictionary\n",
    "def make_pf_value(test_metrics):\n",
    "    pf_value={}\n",
    "   \n",
    "    y= test_metrics[0]\n",
    "    scores= test_metrics[1]\n",
    "    \n",
    "    pf_value['acc']= accuracy_score(y, scores>0.5)\n",
    "    pf_value['f1']= f1_score(y, scores>0.5)\n",
    "    pf_value['precision']= precision_score(y, scores>0.5)\n",
    "    pf_value['recall']= recall_score(y, scores>0.5)\n",
    "    pf_value['auroc']= roc_auc_score(y, scores)\n",
    "    pf_value['auprc']= average_precision_score(y, scores>0.5)\n",
    "    tp = np.sum(np.logical_and(y == 1, scores>0.5))\n",
    "    tn = np.sum(np.logical_and(y == 0, scores<=0.5))\n",
    "    fp = np.sum(np.logical_and(y == 0, scores>0.5))\n",
    "    fn = np.sum(np.logical_and(y == 1, scores<=0.5))\n",
    "    pf_value['sensitivity']= tp/(tp+fn)\n",
    "    pf_value['specificity']= tn/(fp+tn)\n",
    "    pf_value['fpr']= fp/(tp+fp)\n",
    "    pf_value['fnr']= fn/(tn+fn)\n",
    "    \n",
    "    return pf_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93cff-0993-4554-af68-d14ecf9565b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply youden's value\n",
    "def make_youdens_value(test_metrics):\n",
    "    youdens_value={}\n",
    "   \n",
    "    y= test_metrics[0]\n",
    "    scores= test_metrics[1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y, scores)\n",
    "    j = tpr - fpr\n",
    "    thresholds = thresholds[thresholds < 1]\n",
    "    ix = np.argmax(j)\n",
    "    youdens_thr = thresholds[ix]\n",
    "    \n",
    "    youdens_value['acc']= accuracy_score(y, (scores> youdens_thr))\n",
    "    youdens_value['f1']= f1_score(y,  (scores> youdens_thr))\n",
    "    youdens_value['precision']= precision_score(y,  (scores> youdens_thr))\n",
    "    youdens_value['recall']= recall_score(y,  (scores> youdens_thr))\n",
    "    youdens_value['auroc']= roc_auc_score(y,  (scores> youdens_thr))\n",
    "    youdens_value['auprc']= average_precision_score(y, (scores> youdens_thr))\n",
    "    tp = np.sum(np.logical_and(y == 1, scores> youdens_thr))\n",
    "    tn = np.sum(np.logical_and(y == 0, scores<= youdens_thr))\n",
    "    fp = np.sum(np.logical_and(y == 0, scores> youdens_thr))\n",
    "    fn = np.sum(np.logical_and(y == 1, scores<= youdens_thr))\n",
    "    youdens_value['sensitivity']= tp/(tp+fn)\n",
    "    youdens_value['specificity']= tn/(fp+tn)\n",
    "    youdens_value['fpr']= fp/(tp+fp)\n",
    "    youdens_value['fnr']= fn/(tn+fn)\n",
    "    \n",
    "    return youdens_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b9b15-4771-4735-a766-16a53006f1b1",
   "metadata": {},
   "source": [
    "## Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1849b90-4e60-46dd-893f-16b5c3a8347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_pf(fd, loaded_model, metrics_dict, pf_dict, youdens_dict):\n",
    "\n",
    "    test_gen= EchoDataGen(\n",
    "            get_test_info(label_csv, fd),\n",
    "            balance= False,\n",
    "            ratio_int= None,\n",
    "            aug_bool= False,\n",
    "            batch_size= 32,\n",
    "            rvi= True,\n",
    "            sax= True,\n",
    "            #seg= True,\n",
    "        )\n",
    "    test_metrics= get_metrics(test_gen, loaded_model)\n",
    "    metrics_dict[fd]= test_metrics\n",
    "    pf_dict[fd]= make_pf_value(test_metrics)\n",
    "    youdens_dict[fd]= make_youdens_value(test_metrics)\n",
    "\n",
    "metrics_dict={}\n",
    "pf_dict={}\n",
    "youdens_dict={}\n",
    "return_pf(0, loaded_model, metrics_dict, pf_dict, youdens_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b866b1-ec0e-40ca-ba2f-af351b5d6bf7",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "### Average the performance Dict for all 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c48fe0-de95-429a-acc7-90a0b1961310",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict={}\n",
    "pf_dict={}\n",
    "youdens_dict={}\n",
    "return_pf(0, loaded_model_0, metrics_dict, pf_dict, youdens_dict)\n",
    "return_pf(1, loaded_model_1, metrics_dict, pf_dict, youdens_dict)\n",
    "return_pf(2, loaded_model_2, metrics_dict, pf_dict, youdens_dict)\n",
    "return_pf(3, loaded_model_3, metrics_dict, pf_dict, youdens_dict)\n",
    "return_pf(4, loaded_model_4, metrics_dict, pf_dict, youdens_dict)\n",
    "avg_pf={}\n",
    "for k in pf_dict[0].keys():\n",
    "    avg_pf[k]=0\n",
    "    for i in range(len(pf_dict)):\n",
    "        avg_pf[k]+=pf_dict[i][k]\n",
    "    avg_pf[k]/=5\n",
    "    \n",
    "avg_youdens={}\n",
    "for k in youdens_dict[0].keys():\n",
    "    avg_youdens[k]=0\n",
    "    for i in range(len(youdens_dict)):\n",
    "        avg_youdens[k]+=youdens_dict[i][k]\n",
    "    avg_youdens[k]/=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9b54d-7e9a-47ee-8f88-ba23da3530d0",
   "metadata": {},
   "source": [
    "## Draw Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5f194-9552-4a03-88e9-39ed36d8245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= metrics_dict[0][0]\n",
    "scores=  metrics_dict[0][1]\n",
    "ConfusionMatrixDisplay.from_predictions(y, (scores> 0.5), cmap= 'Oranges')\n",
    "accuracy= accuracy_score(y, scores>0.5)\n",
    "plt.title(\"Threshold 0.5: accuracy {}\".format(accuracy))\n",
    "#plt.savefig(r'C:\\Users\\User\\Desktop\\fig1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa23adc-6624-4985-85b1-a8691ec43e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "youdens_value={}\n",
    "test_metrics= metrics_dict[0]\n",
    "y= test_metrics[0]\n",
    "scores= test_metrics[1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, scores)\n",
    "j = tpr - fpr\n",
    "thresholds = thresholds[thresholds < 1]\n",
    "ix = np.argmax(j)\n",
    "youdens_thr = thresholds[ix]\n",
    "youdens_accuracy= accuracy_score(y, scores> youdens_thr)\n",
    "ConfusionMatrixDisplay.from_predictions(y, (scores> youdens_thr), cmap= 'Blues')\n",
    "plt.title(\"Optimal threshold: accuracy {}\".format(youdens_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d702e-53be-4765-aee5-9835728feabf",
   "metadata": {},
   "source": [
    "## Draw AUROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee3833-4397-4000-93f1-8dd119f6dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(metrics_dict[i][0], metrics_dict[i][1])\n",
    "    \n",
    "    plt.plot(fpr, tpr, 'b', alpha=0.15)\n",
    "    tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "dev= st.tstd(np.array(pf_list_dict['auroc']))\n",
    "    \n",
    "\n",
    "plt.plot(base_fpr, mean_tprs, color= 'b', label= f\"Average AUROC={round((avg_pf['auroc']), 3)} 95%CI=[{round((avg_pf['auroc']-1.96*dev), 3)},{round((avg_pf['auroc']+1.96*dev), 3)}]\")\n",
    "\n",
    "\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "plt.legend(loc= 'lower right', fontsize= 'small')\n",
    "plt.plot([0, 1], [0, 1],'darkorange', linestyle= 'dashed')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig(r\"./A4C_AUROC.jpg\", dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
